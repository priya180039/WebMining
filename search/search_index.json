{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Identitas Nama Priya Bagus Amanullah Nim 180411100039 Kelas Web Mining Kelas A Jurusan Teknik Informatika Dosen Pengampu Mula'ab, S.Si., M.Kom.","title":"Identitas"},{"location":"#identitas","text":"","title":"Identitas"},{"location":"#nama","text":"Priya Bagus Amanullah","title":"Nama"},{"location":"#nim","text":"180411100039","title":"Nim"},{"location":"#kelas","text":"Web Mining Kelas A","title":"Kelas"},{"location":"#jurusan","text":"Teknik Informatika","title":"Jurusan"},{"location":"#dosen-pengampu","text":"Mula'ab, S.Si., M.Kom.","title":"Dosen Pengampu"},{"location":"2crawling/","text":"Crawling Teori \u200b Web crawling adalah proses saat kita mengunjungi suatu website lalu kita mencari data, mengindeks setiap kata yang ada. Contoh penggunaan crawling sama seperti sebuah mesin pencarian yang mengindeks situs tertentu agar sesuai dengan apa yang dicari. Lalu web scraping adalah proses untuk mengekstrak data tertentu pada suatu situs ke dalam format yang bisa dianalisis. Contohnya saat kita ingin mengambil sejumlah data yang ada pada suatu situs lalu kita ingin menganalisis data spesifik yang tersedia. \u200b Dalam mengimplementasi web crawling, saya menggunakan bahasa pemrograman python dengan library scrapy. Scrapy adalah framework open-source untuk web crawling untuk bahasa pemrograman python. Code pada CMD \u200b Yang harus dilakukan pertama yaitu menginstall scrapy pada cmd. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>pip install Scrapy \u200b Setelah menginstall scrapy, selanjutnya kita dapat menggunakan scrapy, kita mulai dengan startproject. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>scrapy startproject scrape1 \u200b Setelah itu akan muncul text: You can start your first spider with: cd scrape1 scrapy genspider example example.com \u200b Kita masukan \"cd scrape1\" pada cmd untuk masuk ke direktori scrape1. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>cd scrape1 \u200b Setelah itu, kita inputkan \"scrapy genspider [nama website] [link website]\". C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>scrapy genspider uniqlo uniqlo.com/id/ \u200b Maka akan muncul text: Created spider 'uniqlo' using template 'basic' in module: scrape1.spiders.uniqlo \u200b Dengan begitu sudah terbentuk spider uniqlo, sekarang ke file explorer ke direktori C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1\\scrape1\\spiders, maka akan ada file python bernama uniqlo.py. Kita dapat mengubah isinya agar bisa crawling pada website tersebut. Code pada IDLE Python import scrapy class UniqloSpider(scrapy.Spider): name = 'uniqlo' allowed_domains = ['https://www.uniqlo.com/id/'] start_urls = ['https://www.uniqlo.com/id/product/men/tops/jacketcoat'] def parse(self, response): data = response.css('.unit') for item in data: productname = item.css('.name a::text').extract() productprice = item.css('.price span::text').extract() yield{ 'Nama Produk' : productname, 'Harga Produk' : productprice } \u200b Lalu untuk menjalankan programnya, kita kembali ke cmd untuk menginputkan code. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1>scrapy crawl uniqlo \u200b Maka akan keluar result yang sesuai dengan isi code yang kita edit pada python text editor. Jika ingin memasukkan data tersebut ke dalam format yang bisa dianalisis, kita masukkan code C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1>scrapy crawl uniqlo -o result.csv \u200b Dengan begitu akan terbentuk file output dengan format csv yang bernama result.csv.","title":"Crawling"},{"location":"2crawling/#crawling","text":"","title":"Crawling"},{"location":"2crawling/#teori","text":"\u200b Web crawling adalah proses saat kita mengunjungi suatu website lalu kita mencari data, mengindeks setiap kata yang ada. Contoh penggunaan crawling sama seperti sebuah mesin pencarian yang mengindeks situs tertentu agar sesuai dengan apa yang dicari. Lalu web scraping adalah proses untuk mengekstrak data tertentu pada suatu situs ke dalam format yang bisa dianalisis. Contohnya saat kita ingin mengambil sejumlah data yang ada pada suatu situs lalu kita ingin menganalisis data spesifik yang tersedia. \u200b Dalam mengimplementasi web crawling, saya menggunakan bahasa pemrograman python dengan library scrapy. Scrapy adalah framework open-source untuk web crawling untuk bahasa pemrograman python.","title":"Teori"},{"location":"2crawling/#code-pada-cmd","text":"\u200b Yang harus dilakukan pertama yaitu menginstall scrapy pada cmd. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>pip install Scrapy \u200b Setelah menginstall scrapy, selanjutnya kita dapat menggunakan scrapy, kita mulai dengan startproject. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>scrapy startproject scrape1 \u200b Setelah itu akan muncul text: You can start your first spider with: cd scrape1 scrapy genspider example example.com \u200b Kita masukan \"cd scrape1\" pada cmd untuk masuk ke direktori scrape1. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>cd scrape1 \u200b Setelah itu, kita inputkan \"scrapy genspider [nama website] [link website]\". C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>scrapy genspider uniqlo uniqlo.com/id/ \u200b Maka akan muncul text: Created spider 'uniqlo' using template 'basic' in module: scrape1.spiders.uniqlo \u200b Dengan begitu sudah terbentuk spider uniqlo, sekarang ke file explorer ke direktori C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1\\scrape1\\spiders, maka akan ada file python bernama uniqlo.py. Kita dapat mengubah isinya agar bisa crawling pada website tersebut.","title":"Code pada CMD"},{"location":"2crawling/#code-pada-idle-python","text":"import scrapy class UniqloSpider(scrapy.Spider): name = 'uniqlo' allowed_domains = ['https://www.uniqlo.com/id/'] start_urls = ['https://www.uniqlo.com/id/product/men/tops/jacketcoat'] def parse(self, response): data = response.css('.unit') for item in data: productname = item.css('.name a::text').extract() productprice = item.css('.price span::text').extract() yield{ 'Nama Produk' : productname, 'Harga Produk' : productprice } \u200b Lalu untuk menjalankan programnya, kita kembali ke cmd untuk menginputkan code. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1>scrapy crawl uniqlo \u200b Maka akan keluar result yang sesuai dengan isi code yang kita edit pada python text editor. Jika ingin memasukkan data tersebut ke dalam format yang bisa dianalisis, kita masukkan code C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts\\scrape1>scrapy crawl uniqlo -o result.csv \u200b Dengan begitu akan terbentuk file output dengan format csv yang bernama result.csv.","title":"Code pada IDLE Python"},{"location":"3textpreprocessing/","text":"Text Preprocessing Teori Text Preprocessing \u200b Text Preprocessing adalah tahapan menyeleksi data untuk membuat data yang akan kita proses lebih terstruktur. Tahapan yang biasanya dilakukan ketika melakukan Text Processing adalah sebagai berikut, Case Folding, Filtering, Tokenization, Stemming, Stopword Removal, Reduksi Dimensi. Teori Case Folding \u200b Case Folding ini dilakukan ketika adanya kesalahan dalam penulisan huruf kapital sehingga harus dirapihkan agar mudah dalam melakukan pemrosesan text. Untuk merapihkan kesalahan penggunaan huruf kapital itu, maka dalam tahap Case Folding setiap huruf diubah ke lowercase atau huruf kecil. Code Case Folding sentences_list = ['Priya Bagus Amanullah adalah mahasiswa yang berasal dari Jakarta.', 'Dia anak pertama dari 3 bersaudara.', 'Dia lulusan dari SMP 84 Jakarta Utara dan SMA 75 Jakarta.', 'Priya Bagus Amanullah sedang menempuh pendidikan kuliah di Universitas Trunojoyo Madura, yang saat ini sedang menjalani semester ke-6 nya.'] sentences_temp = [] for a in sentences_list: hasil = a.lower() sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) Hasil output setelah dilakukannya Case Folding, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta.', 'dia anak pertama dari 3 bersaudara.', 'dia lulusan dari smp 84 jakarta utara dan sma 75 jakarta.', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura, yang saat ini sedang menjalani semester ke-6 nya.'] Teori Filtering \u200b Filtering ini dilakukan ketika adanya simbol-simbol atau karakter lain selain huruf sehingga harus dirapihkan agar mudah dalam melakukan pemrosesan text. Untuk merapihkan karakter itu, maka dalam tahap Filtering akan menghilangkan karakter seperti angka atau tanda baca. Code Filtering Data melanjutkan dari result Case Folding tadi import re sentences_temp = [] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) Hasil output setelah dilakukannya Filtering angka, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta.', 'dia anak pertama dari bersaudara.', 'dia lulusan dari smp jakarta utara dan sma jakarta.', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura, yang saat ini sedang menjalani semester ke- nya.'] Data melanjutkan dari result Filtering angka import string sentences_temp = [] for i in sentences_list: hasil = i.translate(str.maketrans(\"\",\"\",string.punctuation)) sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) len(sentences_list) Hasil output setelah dilakukannya Filtering tanda baca, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta', 'dia anak pertama dari bersaudara', 'dia lulusan dari smp jakarta utara dan sma jakarta', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura yang saat ini sedang menjalani semester ke nya'] Teori Tokenization \u200b Setelah merapihkan isi dokumen pada tahapan Case Folding dan Filtering, sekarang tahapan Tokenization adalah untuk memisahkan setiap kata, dan kata-kata yang telah dipisahkan tersebut disebut token. Code Tokenization Untuk melakukan tokenization ini, kita memerlukan library NLTK pada python. Pertama, install dulu nltk. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python39\\Scripts>pip install nltk Lalu install punkt, caranya import nltk nltk.download() Lalu setelah itu akan muncul window, klik models lalu cari punkt lalu klik download. Berikutnya di text editor python Data melanjutkan dari result Filtering temp = '' loop = 0 for sentence in sentences_list: loop += 1 temp += sentence if loop < len(sentences_list): temp += ' ' import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist tokenresult = nltk.tokenize.word_tokenize(temp) tokenfreq = nltk.FreqDist(tokenresult) print(tokenfreq.most_common()) Hasil output setelah Tokenize dan menghitung frekuensinya, sebagai berikut: [('dari', 3), ('jakarta', 3), ('priya', 2), ('bagus', 2), ('amanullah', 2), ('yang', 2), ('dia', 2), ('sedang', 2), ('adalah', 1), ('mahasiswa', 1), ('berasal', 1), ('anak', 1), ('pertama', 1), ('bersaudara', 1), ('lulusan', 1), ('smp', 1), ('utara', 1), ('dan', 1), ('sma', 1), ('menempuh', 1), ('pendidikan', 1), ('kuliah', 1), ('di', 1), ('universitas', 1), ('trunojoyo', 1), ('madura', 1), ('saat', 1), ('ini', 1), ('menjalani', 1), ('semester', 1), ('ke', 1), ('nya', 1)] Teori Stemming \u200b Dilakukan untuk mengubah suatu kata menjadi lebih sederhana yaitu menjadi kata dasar dengan menghilangkan imbuhan prefiks maupun sufiks dan dikelompokkan berdasarkan kata dasar itu. Code Stemming Data melanjutkan dari result Filtering sebelumnya from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() new = [] for x in sentences_list: tempstem = stemmer.stem(x) new.append(tempstem) print(new) Hasil output yang dihasilkan setelah Stemming, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang asal dari jakarta', 'dia anak pertama dari saudara', 'dia lulus dari smp jakarta utara dan sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah di universitas trunojoyo madura yang saat ini sedang jalan semester ke nya'] Teori Stopword Removal \u200b Stopword Removal dilakukan untuk menghilangkan kata-kata yang sekiranya tidak terlalu penting atau tidak terlalu berarti dalam dokumen, dan mengambil kata-kata yang penting. Filter ini dilakukan setelah proses stemming. Code Stopword Removal Data dilanjutkan dari result Stemming sebelumnya from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() dokumen = [] for y in new: afterRemoval = stopword.remove(y) dokumen.append(afterRemoval) print(dokumen) Hasil output setelah Stopword Removal, sebagai berikut: ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya'] Teori Reduksi Dimensi \u200b Dilakukan ketika kita ingin mengurangi dimensi pada suatu dataset yang ingin kita olah. Saat ada banyak fitur atau kolom, setelah dilakukannya reduksi dimensi maka jumlah fitur atau kolom bisa berkurang tanpa harus kehilangan informasi yang tersedia. Contoh penggunaan tahapan ini juga terdapat saat kita ingin mengkompres file yang sangat besar, saat kita mengkompres file tersebut maka ukurannya akan berkurang tanpa kehilangan informasi yang terdapat pada dalam file tersebut. Code Reduksi Dimensi Data melanjutkan dari result Stopword Removal sebelumnya from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer cv = CountVectorizer() bag = cv.fit_transform(dokumen) matrik_vsm=bag.toarray() import pandas as pd a=cv.get_feature_names() print(len(cv.get_feature_names())) from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumen)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) print(dfb) Hasil output matriks TF-IDF amanullah anak asal bagus didik jakarta jalan kuliah lulus madura ... saat saudara sedang semester sma smp tempuh trunojoyo utara 1 0.372225 0.00000 0.47212 0.372225 0.000000 0.372225 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2 0.000000 0.57735 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.57735 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 3 0.000000 0.00000 0.00000 0.000000 0.000000 0.619130 0.000000 0.000000 0.392644 0.000000 ... 0.000000 0.00000 0.000000 0.000000 0.392644 0.392644 0.000000 0.000000 0.392644 4 0.197941 0.00000 0.00000 0.197941 0.251063 0.000000 0.251063 0.251063 0.000000 0.251063 ... 0.251063 0.00000 0.502126 0.251063 0.000000 0.000000 0.251063 0.251063 0.000000 4 rows \u00d7 24 columns Lalu melakukan Reduksi Dimensi menggunakan PCA from sklearn.decomposition import PCA pca = PCA(n_components=2) fit_pca = pca.fit_transform(dfb) pca_df = pd.DataFrame(data = fit_pca, columns = ['PCA_1', 'PCA_2']) pca_df.tail() Hasil output setelah dilakukannya Reduksi Dimensi PCA_1 PCA_2 0 -0.414373 -1.025937e-16 1 0.892074 -2.948689e-02 2 -0.262446 -6.919021e-01","title":"Text Preprocessing"},{"location":"3textpreprocessing/#text-preprocessing","text":"","title":"Text Preprocessing"},{"location":"3textpreprocessing/#teori-text-preprocessing","text":"\u200b Text Preprocessing adalah tahapan menyeleksi data untuk membuat data yang akan kita proses lebih terstruktur. Tahapan yang biasanya dilakukan ketika melakukan Text Processing adalah sebagai berikut, Case Folding, Filtering, Tokenization, Stemming, Stopword Removal, Reduksi Dimensi.","title":"Teori Text Preprocessing"},{"location":"3textpreprocessing/#teori-case-folding","text":"\u200b Case Folding ini dilakukan ketika adanya kesalahan dalam penulisan huruf kapital sehingga harus dirapihkan agar mudah dalam melakukan pemrosesan text. Untuk merapihkan kesalahan penggunaan huruf kapital itu, maka dalam tahap Case Folding setiap huruf diubah ke lowercase atau huruf kecil.","title":"Teori Case Folding"},{"location":"3textpreprocessing/#code-case-folding","text":"sentences_list = ['Priya Bagus Amanullah adalah mahasiswa yang berasal dari Jakarta.', 'Dia anak pertama dari 3 bersaudara.', 'Dia lulusan dari SMP 84 Jakarta Utara dan SMA 75 Jakarta.', 'Priya Bagus Amanullah sedang menempuh pendidikan kuliah di Universitas Trunojoyo Madura, yang saat ini sedang menjalani semester ke-6 nya.'] sentences_temp = [] for a in sentences_list: hasil = a.lower() sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) Hasil output setelah dilakukannya Case Folding, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta.', 'dia anak pertama dari 3 bersaudara.', 'dia lulusan dari smp 84 jakarta utara dan sma 75 jakarta.', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura, yang saat ini sedang menjalani semester ke-6 nya.']","title":"Code Case Folding"},{"location":"3textpreprocessing/#teori-filtering","text":"\u200b Filtering ini dilakukan ketika adanya simbol-simbol atau karakter lain selain huruf sehingga harus dirapihkan agar mudah dalam melakukan pemrosesan text. Untuk merapihkan karakter itu, maka dalam tahap Filtering akan menghilangkan karakter seperti angka atau tanda baca.","title":"Teori Filtering"},{"location":"3textpreprocessing/#code-filtering","text":"Data melanjutkan dari result Case Folding tadi import re sentences_temp = [] for i in sentences_list: hasil = re.sub(r\"\\d+\", \"\", i) sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) Hasil output setelah dilakukannya Filtering angka, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta.', 'dia anak pertama dari bersaudara.', 'dia lulusan dari smp jakarta utara dan sma jakarta.', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura, yang saat ini sedang menjalani semester ke- nya.'] Data melanjutkan dari result Filtering angka import string sentences_temp = [] for i in sentences_list: hasil = i.translate(str.maketrans(\"\",\"\",string.punctuation)) sentences_temp.append(hasil) sentences_list = sentences_temp print(sentences_list) len(sentences_list) Hasil output setelah dilakukannya Filtering tanda baca, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang berasal dari jakarta', 'dia anak pertama dari bersaudara', 'dia lulusan dari smp jakarta utara dan sma jakarta', 'priya bagus amanullah sedang menempuh pendidikan kuliah di universitas trunojoyo madura yang saat ini sedang menjalani semester ke nya']","title":"Code Filtering"},{"location":"3textpreprocessing/#teori-tokenization","text":"\u200b Setelah merapihkan isi dokumen pada tahapan Case Folding dan Filtering, sekarang tahapan Tokenization adalah untuk memisahkan setiap kata, dan kata-kata yang telah dipisahkan tersebut disebut token.","title":"Teori Tokenization"},{"location":"3textpreprocessing/#code-tokenization","text":"Untuk melakukan tokenization ini, kita memerlukan library NLTK pada python. Pertama, install dulu nltk. C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python39\\Scripts>pip install nltk Lalu install punkt, caranya import nltk nltk.download() Lalu setelah itu akan muncul window, klik models lalu cari punkt lalu klik download. Berikutnya di text editor python Data melanjutkan dari result Filtering temp = '' loop = 0 for sentence in sentences_list: loop += 1 temp += sentence if loop < len(sentences_list): temp += ' ' import nltk from nltk.tokenize import word_tokenize from nltk.probability import FreqDist tokenresult = nltk.tokenize.word_tokenize(temp) tokenfreq = nltk.FreqDist(tokenresult) print(tokenfreq.most_common()) Hasil output setelah Tokenize dan menghitung frekuensinya, sebagai berikut: [('dari', 3), ('jakarta', 3), ('priya', 2), ('bagus', 2), ('amanullah', 2), ('yang', 2), ('dia', 2), ('sedang', 2), ('adalah', 1), ('mahasiswa', 1), ('berasal', 1), ('anak', 1), ('pertama', 1), ('bersaudara', 1), ('lulusan', 1), ('smp', 1), ('utara', 1), ('dan', 1), ('sma', 1), ('menempuh', 1), ('pendidikan', 1), ('kuliah', 1), ('di', 1), ('universitas', 1), ('trunojoyo', 1), ('madura', 1), ('saat', 1), ('ini', 1), ('menjalani', 1), ('semester', 1), ('ke', 1), ('nya', 1)]","title":"Code Tokenization"},{"location":"3textpreprocessing/#teori-stemming","text":"\u200b Dilakukan untuk mengubah suatu kata menjadi lebih sederhana yaitu menjadi kata dasar dengan menghilangkan imbuhan prefiks maupun sufiks dan dikelompokkan berdasarkan kata dasar itu.","title":"Teori Stemming"},{"location":"3textpreprocessing/#code-stemming","text":"Data melanjutkan dari result Filtering sebelumnya from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() new = [] for x in sentences_list: tempstem = stemmer.stem(x) new.append(tempstem) print(new) Hasil output yang dihasilkan setelah Stemming, sebagai berikut: ['priya bagus amanullah adalah mahasiswa yang asal dari jakarta', 'dia anak pertama dari saudara', 'dia lulus dari smp jakarta utara dan sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah di universitas trunojoyo madura yang saat ini sedang jalan semester ke nya']","title":"Code Stemming"},{"location":"3textpreprocessing/#teori-stopword-removal","text":"\u200b Stopword Removal dilakukan untuk menghilangkan kata-kata yang sekiranya tidak terlalu penting atau tidak terlalu berarti dalam dokumen, dan mengambil kata-kata yang penting. Filter ini dilakukan setelah proses stemming.","title":"Teori Stopword Removal"},{"location":"3textpreprocessing/#code-stopword-removal","text":"Data dilanjutkan dari result Stemming sebelumnya from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() dokumen = [] for y in new: afterRemoval = stopword.remove(y) dokumen.append(afterRemoval) print(dokumen) Hasil output setelah Stopword Removal, sebagai berikut: ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya']","title":"Code Stopword Removal"},{"location":"3textpreprocessing/#teori-reduksi-dimensi","text":"\u200b Dilakukan ketika kita ingin mengurangi dimensi pada suatu dataset yang ingin kita olah. Saat ada banyak fitur atau kolom, setelah dilakukannya reduksi dimensi maka jumlah fitur atau kolom bisa berkurang tanpa harus kehilangan informasi yang tersedia. Contoh penggunaan tahapan ini juga terdapat saat kita ingin mengkompres file yang sangat besar, saat kita mengkompres file tersebut maka ukurannya akan berkurang tanpa kehilangan informasi yang terdapat pada dalam file tersebut.","title":"Teori Reduksi Dimensi"},{"location":"3textpreprocessing/#code-reduksi-dimensi","text":"Data melanjutkan dari result Stopword Removal sebelumnya from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer cv = CountVectorizer() bag = cv.fit_transform(dokumen) matrik_vsm=bag.toarray() import pandas as pd a=cv.get_feature_names() print(len(cv.get_feature_names())) from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True,norm='l2',smooth_idf=True) tf=tfidf.fit_transform(cv.fit_transform(dokumen)).toarray() dfb =pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a]) print(dfb) Hasil output matriks TF-IDF amanullah anak asal bagus didik jakarta jalan kuliah lulus madura ... saat saudara sedang semester sma smp tempuh trunojoyo utara 1 0.372225 0.00000 0.47212 0.372225 0.000000 0.372225 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 2 0.000000 0.57735 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 ... 0.000000 0.57735 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 3 0.000000 0.00000 0.00000 0.000000 0.000000 0.619130 0.000000 0.000000 0.392644 0.000000 ... 0.000000 0.00000 0.000000 0.000000 0.392644 0.392644 0.000000 0.000000 0.392644 4 0.197941 0.00000 0.00000 0.197941 0.251063 0.000000 0.251063 0.251063 0.000000 0.251063 ... 0.251063 0.00000 0.502126 0.251063 0.000000 0.000000 0.251063 0.251063 0.000000 4 rows \u00d7 24 columns Lalu melakukan Reduksi Dimensi menggunakan PCA from sklearn.decomposition import PCA pca = PCA(n_components=2) fit_pca = pca.fit_transform(dfb) pca_df = pd.DataFrame(data = fit_pca, columns = ['PCA_1', 'PCA_2']) pca_df.tail() Hasil output setelah dilakukannya Reduksi Dimensi PCA_1 PCA_2 0 -0.414373 -1.025937e-16 1 0.892074 -2.948689e-02 2 -0.262446 -6.919021e-01","title":"Code Reduksi Dimensi"},{"location":"4modelling/","text":"Modelling Teori Text Classification \u200b Text Classification adalah proses untuk mengelompokkan atau mengklasifikasikan kata-kata yang ada ke berbagai kategori yang disediakan. Saya akan mengklasifikasikan kata menggunakan LSA (Latent Semantic Analysis). LSA adalah teknik atau cara dalam merepresentasikan vektor dari sebuah dokumen yang disediakan. Code Text Classification Data yang digunakan adalah text document yang telah melewati preprocessing (Case Folding, Filtering, Stemming, Stopword Removal). doc_complete = ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya'] print(doc_complete) Hasil output: ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya'] from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X =vectorizer.fit_transform(doc_complete) print(X) Hasil output: ``(0, 5) 0.3722248517590162 (0, 2) 0.47212002654617047 (0, 10) 0.47212002654617047 (0, 0) 0.3722248517590162 (0, 3) 0.3722248517590162 (0, 13) 0.3722248517590162 (1, 15) 0.5773502691896257 (1, 12) 0.5773502691896257 (1, 1) 0.5773502691896257 (2, 18) 0.39264413785519836 (2, 23) 0.39264413785519836 (2, 19) 0.39264413785519836 (2, 8) 0.39264413785519836 (2, 5) 0.6191302964899972 (3, 11) 0.2510631654356404 (3, 17) 0.2510631654356404 (3, 6) 0.2510631654356404 (3, 14) 0.2510631654356404 (3, 9) 0.2510631654356404 (3, 21) 0.2510631654356404 (3, 22) 0.2510631654356404 (3, 7) 0.2510631654356404 (3, 4) 0.2510631654356404 (3, 20) 0.2510631654356404 (3, 16) 0.5021263308712808 (3, 0) 0.19794108337255964 (3, 3) 0.19794108337255964 (3, 13) 0.19794108337255964 from sklearn.decomposition import TruncatedSVD lsa = TruncatedSVD(n_components=2,n_iter=100) print(lsa.fit(X)) Hasil output: TruncatedSVD(n_iter=100) terms = vectorizer.get_feature_names() for i,comp in enumerate(lsa.components_): termsInComp = zip(terms,comp) sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10] print(\"Concept %d:\" % i) for term in sortedterms: print(term[0]) print(\" \") Hasil output yang merupakan final result yang menunjukkan hasil klasifikasi kata-kata yang terdapat pada dokumen: Concept 0: jakarta amanullah bagus priya asal mahasiswa sedang lulus sma smp Concept 1: pertama saudara anak jakarta lulus sma smp utara mahasiswa asal","title":"Modelling"},{"location":"4modelling/#modelling","text":"","title":"Modelling"},{"location":"4modelling/#teori-text-classification","text":"\u200b Text Classification adalah proses untuk mengelompokkan atau mengklasifikasikan kata-kata yang ada ke berbagai kategori yang disediakan. Saya akan mengklasifikasikan kata menggunakan LSA (Latent Semantic Analysis). LSA adalah teknik atau cara dalam merepresentasikan vektor dari sebuah dokumen yang disediakan.","title":"Teori Text Classification"},{"location":"4modelling/#code-text-classification","text":"Data yang digunakan adalah text document yang telah melewati preprocessing (Case Folding, Filtering, Stemming, Stopword Removal). doc_complete = ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya'] print(doc_complete) Hasil output: ['priya bagus amanullah mahasiswa asal jakarta', 'anak pertama saudara', 'lulus smp jakarta utara sma jakarta', 'priya bagus amanullah sedang tempuh didik kuliah universitas trunojoyo madura saat sedang jalan semester nya'] from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() X =vectorizer.fit_transform(doc_complete) print(X) Hasil output: ``(0, 5) 0.3722248517590162 (0, 2) 0.47212002654617047 (0, 10) 0.47212002654617047 (0, 0) 0.3722248517590162 (0, 3) 0.3722248517590162 (0, 13) 0.3722248517590162 (1, 15) 0.5773502691896257 (1, 12) 0.5773502691896257 (1, 1) 0.5773502691896257 (2, 18) 0.39264413785519836 (2, 23) 0.39264413785519836 (2, 19) 0.39264413785519836 (2, 8) 0.39264413785519836 (2, 5) 0.6191302964899972 (3, 11) 0.2510631654356404 (3, 17) 0.2510631654356404 (3, 6) 0.2510631654356404 (3, 14) 0.2510631654356404 (3, 9) 0.2510631654356404 (3, 21) 0.2510631654356404 (3, 22) 0.2510631654356404 (3, 7) 0.2510631654356404 (3, 4) 0.2510631654356404 (3, 20) 0.2510631654356404 (3, 16) 0.5021263308712808 (3, 0) 0.19794108337255964 (3, 3) 0.19794108337255964 (3, 13) 0.19794108337255964 from sklearn.decomposition import TruncatedSVD lsa = TruncatedSVD(n_components=2,n_iter=100) print(lsa.fit(X)) Hasil output: TruncatedSVD(n_iter=100) terms = vectorizer.get_feature_names() for i,comp in enumerate(lsa.components_): termsInComp = zip(terms,comp) sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10] print(\"Concept %d:\" % i) for term in sortedterms: print(term[0]) print(\" \") Hasil output yang merupakan final result yang menunjukkan hasil klasifikasi kata-kata yang terdapat pada dokumen: Concept 0: jakarta amanullah bagus priya asal mahasiswa sedang lulus sma smp Concept 1: pertama saudara anak jakarta lulus sma smp utara mahasiswa asal","title":"Code Text Classification"},{"location":"5evaluasi/","text":"Evaluasi Teori Evaluasi \u200b Dalam meringkas dokumen, untuk meringkas dokumen secara otomatis, paling banyak menggunakan Rouge. Rouge menghitung Precison, Recall dan F-measure untuk mengukur keakuratan hasil ringkasan yang dibuat. Code Evaluasi Install terlebih dahulu rouge di cmd C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>pip install rouge-score Berikut adalah contoh code dalam evaluasi Rouge: from rouge_score import rouge_scorer hypothesis = ['Gilgamesh was a supreme, he was two-thirds god and one-third human.', 'He was an unstoppable despot with supreme divinity.'] reference = ['Lugalbanda, the King of Uruk, and Rimat-Ninsun, the goddess, gave birth to a king of heroes, Gilgamesh.', \"He was the King of Heroes, possessing all of the world's treasures.\"] scorer = rouge_scorer.RougeScorer(['rouge1']) results = {'precision': [], 'recall': [], 'fmeasure': []} for (h, r) in zip(hyp, ref): score = scorer.score(h, r) precision, recall, fmeasure = score['rouge1'] results['precision'].append(precision) results['recall'].append(recall) results['fmeasure'].append(fmeasure) print(results['precision']) print(results['recall']) print(results['fmeasure']) Berikut adalah outputnya: [0.16666666666666666, 0.15384615384615385] [0.23076923076923078, 0.25] [0.1935483870967742, 0.1904761904761905]","title":"Evaluasi"},{"location":"5evaluasi/#evaluasi","text":"","title":"Evaluasi"},{"location":"5evaluasi/#teori-evaluasi","text":"\u200b Dalam meringkas dokumen, untuk meringkas dokumen secara otomatis, paling banyak menggunakan Rouge. Rouge menghitung Precison, Recall dan F-measure untuk mengukur keakuratan hasil ringkasan yang dibuat.","title":"Teori Evaluasi"},{"location":"5evaluasi/#code-evaluasi","text":"Install terlebih dahulu rouge di cmd C:\\Users\\Win_10\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts>pip install rouge-score Berikut adalah contoh code dalam evaluasi Rouge: from rouge_score import rouge_scorer hypothesis = ['Gilgamesh was a supreme, he was two-thirds god and one-third human.', 'He was an unstoppable despot with supreme divinity.'] reference = ['Lugalbanda, the King of Uruk, and Rimat-Ninsun, the goddess, gave birth to a king of heroes, Gilgamesh.', \"He was the King of Heroes, possessing all of the world's treasures.\"] scorer = rouge_scorer.RougeScorer(['rouge1']) results = {'precision': [], 'recall': [], 'fmeasure': []} for (h, r) in zip(hyp, ref): score = scorer.score(h, r) precision, recall, fmeasure = score['rouge1'] results['precision'].append(precision) results['recall'].append(recall) results['fmeasure'].append(fmeasure) print(results['precision']) print(results['recall']) print(results['fmeasure']) Berikut adalah outputnya: [0.16666666666666666, 0.15384615384615385] [0.23076923076923078, 0.25] [0.1935483870967742, 0.1904761904761905]","title":"Code Evaluasi"}]}